import openai
from openai import OpenAI
import sounddevice as sd
import scipy.io.wavfile as wav
import tempfile
import os
from playsound import playsound  # mp3 ì¬ìƒìš©

# OpenAI API
os.environ["OPENAI_API_KEY"] = (
    ""
)
client = OpenAI()

initial_prompt = """
ë‹¹ì‹ ì€ ì¹´í˜ì˜ ì¹œì ˆí•œ ì§ì›ì…ë‹ˆë‹¤.
ë‹¤ìŒ ìˆœì„œì— ë”°ë¼ì„œ ì£¼ë¬¸ì„ ë°›ìœ¼ë©´ ë©ë‹ˆë‹¤.

[1ë‹¨ê³„]
- [ì•„ë©”ë¦¬ì¹´ë…¸, ì¹´í˜ë¼ë–¼, ì¹´í‘¸ì¹˜ë…¸] ë©”ë‰´ ì¤‘ í•˜ë‚˜ë¥¼ ì£¼ë¬¸ë°›ìœ¼ë©´ ë©ë‹ˆë‹¤.

[2ë‹¨ê³„]
- [ì•„ì´ìŠ¤, í•«] ì¤‘ í•˜ë‚˜ë¥¼ ì£¼ë¬¸ë°›ìœ¼ì„¸ìš”.

[3ë‹¨ê³„]
{ì•„ë©”ë¦¬ì¹´ë…¸:2500, ì¹´í˜ë¼ë–¼:3000, ì¹´í‘¸ì¹˜ë…¸: 3500}
- ìœ„ ê¸ˆì•¡ ì±…ì •ì— ë”°ë¼ ì´ ê¸ˆì•¡ì„ ê³„ì‚°í•˜ì„¸ìš”.
- ì£¼ë¬¸ ë‚´ì—­ê³¼ ì´ ê¸ˆì•¡ì„ ë§í•˜ê³  ì£¼ë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ ë¼ê³  ë§í•˜ì„¸ìš”.
- ë§¨ ë§ˆì§€ë§‰ ë§ë¡œ 'ì¢…ë£Œí•©ë‹ˆë‹¤' ë¥¼ ë°˜ë“œì‹œ ë§í•˜ì„¸ìš”.

"""


# TTSë¡œ ìŒì„± ìƒì„± ë° ì¬ìƒ
def speak(text):
    print(f"ğŸ—£ï¸ speak(): {text}")
    response = openai.audio.speech.create(
        model="tts-1",
        voice="shimmer",
        input=text,
    )
    with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as audio_file:
        audio_file.write(response.content)
        audio_file.flush()
        audio_path = audio_file.name

    playsound(audio_path)
    os.remove(audio_path)


# ìŒì„± ë…¹ìŒ
SAMPLERATE = 16000
DURATION = 3


def record_audio():
    print("ğŸ™ï¸ ë…¹ìŒ ì¤‘...")
    audio = sd.rec(
        int(SAMPLERATE * DURATION), samplerate=SAMPLERATE, channels=1, dtype="int16"
    )
    sd.wait()
    return audio


# Whisper API í˜¸ì¶œ (ìµœì‹  openai>=1.0.0 ë°©ì‹)
def transcribe(audio_data):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav:
        wav.write(temp_wav.name, SAMPLERATE, audio_data)
        temp_wav_path = temp_wav.name

    with open(temp_wav_path, "rb") as file:
        transcript = openai.audio.transcriptions.create(
            model="whisper-1", file=file, language="ko"
        )

    os.remove(temp_wav_path)
    return transcript.text


def ask(question, message_history=[], model="o4-mini-2025-04-16"):
    if len(message_history) == 0:
        # ìµœì´ˆ ì§ˆë¬¸
        message_history.append(
            {
                "role": "system",
                "content": initial_prompt,
            }
        )

    # ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
    message_history.append(
        {
            "role": "user",
            "content": question,
        },
    )

    # GPTì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±
    completion = client.chat.completions.create(
        model=model,
        messages=message_history,
    )

    # ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¶”ê°€
    message_history.append(
        {"role": "assistant", "content": completion.choices[0].message.content}
    )

    return message_history


if __name__ == "__main__":
    message_history = ask("ì•ˆë…•í•˜ì„¸ìš”?", message_history=[])
    speak(message_history[-1]["content"])

    while True:
        audio_data = record_audio()
        user_text = transcribe(audio_data)
        print(f"ğŸ“ ì¸ì‹ëœ ë§: {user_text}")
        message_history = ask(user_text, message_history=message_history)
        speak(message_history[-1]["content"])

        if any(
            x in message_history[-1]["content"]
            for x in ["ì¢…ë£Œ", "ë", "ê·¸ë§Œ", "ê²°ì œ", "ì£¼ë¬¸ ë", "ì£¼ë¬¸ ì™„ë£Œ", "ë‹¤ í–ˆì–´"]
        ):
            break
